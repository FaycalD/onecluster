ðŸ‘‹ Welcome to Codespaces! You are on our default image. 
   - It includes runtimes and tools for Python, Node.js, Docker, and more. See the full list here: https://aka.ms/ghcs-default-image
   - Want to use a custom image instead? Learn more here: https://aka.ms/configure-codespace

ðŸ” To explore VS Code to its fullest, search using the Command Palette (Cmd/Ctrl + Shift + P or F1).

ðŸ“ Edit away, run your app as usual, and we'll automatically make it available for you to access.

@FaycalD âžœ /workspaces/onecluster (main) $ ssh-keygen -f ~/.ssh/tcloud -t ed25519
Generating public/private ed25519 key pair.
Created directory '/home/codespace/.ssh'.
Enter passphrase (empty for no passphrase): 
Enter same passphrase again: 
Your identification has been saved in /home/codespace/.ssh/tcloud
Your public key has been saved in /home/codespace/.ssh/tcloud.pub
The key fingerprint is:
SHA256:Yv8aYz/X4uwRs0AG4bEayhoL0V0jNM5DkDyf/XNfEk8 codespace@codespaces-c36a76
The key's randomart image is:
+--[ED25519 256]--+
| ..+= o +.       |
| .+= + o +       |
|. .o=o. o o      |
| . .oo.o o  . E  |
|. . o +.S . o+   |
| . + . oo ...+o  |
|  o     =o .o+   |
|       . =..+..  |
|        ..o=+.   |
+----[SHA256]-----+
@FaycalD âžœ /workspaces/onecluster (main) $ cat ~/.ssh/tcloud.pub
ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAICLg9IV4nnFnKPn5cdzs3Zjv8NExygONST6vd0i2rj/T codespace@codespaces-c36a76
@FaycalD âžœ /workspaces/onecluster (main) $ ./run.sh init base
./run.sh: line 7: terraform: command not found
@FaycalD âžœ /workspaces/onecluster (main) $ wget -O- https://apt.releases.hashicorp.com/gpg | sudo gpg --dearmor -o /usr/share/keyrings/hashicorp-archive-keyring.gpg
are/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main" | sudo tee /etc/apt/sources.list.d/hashicorp.list
sudo apt update && sudo apt install terraform--2023-11-09 00:07:43--  https://apt.releases.hashicorp.com/gpg
Resolving apt.releases.hashicorp.com (apt.releases.hashicorp.com)... 18.244.179.53, 18.244.179.89, 18.244.179.43, ...
Connecting to apt.releases.hashicorp.com (apt.releases.hashicorp.com)|18.244.179.53|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 3980 (3.9K) [binary/octet-stream]
Saving to: â€˜STDOUTâ€™

-                                        100%[=================================================================================>]   3.89K  --.-KB/s    in 0s      

2023-11-09 00:07:43 (601 MB/s) - written to stdout [3980/3980]

@FaycalD âžœ /workspaces/onecluster (main) $ echo "deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main" | sudo tee /etc/apt/sources.list.d/hashicorp.list
deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com focal main
@FaycalD âžœ /workspaces/onecluster (main) $ sudo apt update && sudo apt install terraform
Get:1 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]
Get:2 http://archive.ubuntu.com/ubuntu focal InRelease [265 kB]                                                                                                   
Get:3 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]                                                                 
Get:4 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]                                                               
Get:5 https://apt.releases.hashicorp.com focal InRelease [12.9 kB]                                                                      
Get:6 https://dl.yarnpkg.com/debian stable InRelease [17.1 kB]                                                                         
Get:7 https://packages.microsoft.com/repos/microsoft-ubuntu-focal-prod focal InRelease [3611 B]                  
Get:8 https://repo.anaconda.com/pkgs/misc/debrepo/conda stable InRelease [3960 B]
Get:9 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [3194 kB]
Get:10 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [3017 kB]  
Get:11 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [29.3 kB]            
Get:12 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1124 kB]           
Get:13 http://archive.ubuntu.com/ubuntu focal/universe amd64 Packages [11.3 MB]                     
Get:14 http://archive.ubuntu.com/ubuntu focal/main amd64 Packages [1275 kB]                               
Get:15 http://archive.ubuntu.com/ubuntu focal/restricted amd64 Packages [33.4 kB]
Get:16 http://archive.ubuntu.com/ubuntu focal/multiverse amd64 Packages [177 kB]
Get:18 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [32.0 kB]
Get:19 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3683 kB]
Get:20 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1427 kB]
Get:21 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [3166 kB]
Get:17 https://packagecloud.io/github/git-lfs/ubuntu focal InRelease [25.7 kB]
Get:22 http://archive.ubuntu.com/ubuntu focal-backports/universe amd64 Packages [28.6 kB]
Get:23 http://archive.ubuntu.com/ubuntu focal-backports/main amd64 Packages [55.2 kB]          
Get:24 https://apt.releases.hashicorp.com focal/main amd64 Packages [131 kB]                   
Get:25 https://dl.yarnpkg.com/debian stable/main all Packages [11.1 kB]
Get:26 https://dl.yarnpkg.com/debian stable/main amd64 Packages [11.1 kB]
Get:27 https://packages.microsoft.com/repos/microsoft-ubuntu-focal-prod focal/main all Packages [2647 B]
Get:28 https://packages.microsoft.com/repos/microsoft-ubuntu-focal-prod focal/main amd64 Packages [247 kB]
Get:29 https://repo.anaconda.com/pkgs/misc/debrepo/conda stable/main amd64 Packages [3308 B]
Get:30 https://packagecloud.io/github/git-lfs/ubuntu focal/main amd64 Packages [3322 B]
Fetched 29.7 MB in 3s (11.6 MB/s)
Reading package lists... Done
Building dependency tree       
Reading state information... Done
25 packages can be upgraded. Run 'apt list --upgradable' to see them.
Reading package lists... Done
Building dependency tree       
Reading state information... Done
The following NEW packages will be installed:
  terraform
0 upgraded, 1 newly installed, 0 to remove and 25 not upgraded.
Need to get 25.6 MB of archives.
After this operation, 80.7 MB of additional disk space will be used.
Get:1 https://apt.releases.hashicorp.com focal/main amd64 terraform amd64 1.6.3-1 [25.6 MB]
Fetched 25.6 MB in 0s (94.7 MB/s)  
Selecting previously unselected package terraform.
(Reading database ... 68238 files and directories currently installed.)
Preparing to unpack .../terraform_1.6.3-1_amd64.deb ...
Unpacking terraform (1.6.3-1) ...
Setting up terraform (1.6.3-1) ...
@FaycalD âžœ /workspaces/onecluster (main) $ ./run.sh init base

Initializing the backend...
Initializing modules...
- base in ../../modules/base

Initializing provider plugins...
- Finding latest version of hetznercloud/hcloud...
- Installing hetznercloud/hcloud v1.44.1...
- Installed hetznercloud/hcloud v1.44.1 (signed by a HashiCorp partner, key ID 5219EACB3A77198B)

Partner and community providers are signed by their developers.
If you'd like to know more about provider signing, you can read about it here:
https://www.terraform.io/docs/cli/plugins/signing.html

Terraform has created a lock file .terraform.lock.hcl to record the provider
selections it made above. Include this file in your version control repository
so that Terraform can guarantee to make the same selections by default when
you run "terraform init" in the future.

Terraform has been successfully initialized!

You may now begin working with Terraform. Try running "terraform plan" to see
any changes that are required for your infrastructure. All Terraform commands
should now work.

If you ever set or change modules or backend configuration for Terraform,
rerun this command to reinitialize your working directory. If you forget, other
commands will detect it and remind you to do so if necessary.
@FaycalD âžœ /workspaces/onecluster (main) $ ./run.sh deploy base

Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:
  + create

Terraform will perform the following actions:

  # module.base.hcloud_network.private_net will be created
  + resource "hcloud_network" "private_net" {
      + delete_protection        = false
      + expose_routes_to_vswitch = false
      + id                       = (known after apply)
      + ip_range                 = "10.0.0.0/16"
      + labels                   = {
          + "name" = "mycluster-main"
        }
      + name                     = "mycluster-main"
    }

  # module.base.hcloud_network_subnet.private_subnet will be created
  + resource "hcloud_network_subnet" "private_subnet" {
      + gateway      = (known after apply)
      + id           = (known after apply)
      + ip_range     = "10.0.0.0/16"
      + network_id   = (known after apply)
      + network_zone = "eu-central"
      + type         = "server"
    }

  # module.base.hcloud_server.bastion will be created
  + resource "hcloud_server" "bastion" {
      + allow_deprecated_images    = false
      + backup_window              = (known after apply)
      + backups                    = false
      + datacenter                 = (known after apply)
      + delete_protection          = false
      + firewall_ids               = (known after apply)
      + id                         = (known after apply)
      + ignore_remote_firewall_ids = false
      + image                      = "ubuntu-20.04"
      + ipv4_address               = (known after apply)
      + ipv6_address               = (known after apply)
      + ipv6_network               = (known after apply)
      + keep_disk                  = false
      + location                   = "nbg1"
      + name                       = "bastion"
      + rebuild_protection         = false
      + server_type                = "cx31"
      + shutdown_before_deletion   = false
      + ssh_keys                   = [
          + "access1",
        ]
      + status                     = (known after apply)
    }

  # module.base.hcloud_server_network.server_network_bastion will be created
  + resource "hcloud_server_network" "server_network_bastion" {
      + id          = (known after apply)
      + ip          = "10.0.0.2"
      + mac_address = (known after apply)
      + network_id  = (known after apply)
      + server_id   = (known after apply)
    }

Plan: 4 to add, 0 to change, 0 to destroy.

Changes to Outputs:
  + bastion_ip = (known after apply)
module.base.hcloud_network.private_net: Creating...
module.base.hcloud_server.bastion: Creating...
module.base.hcloud_network.private_net: Creation complete after 1s [id=3537577]
module.base.hcloud_network_subnet.private_subnet: Creating...
module.base.hcloud_network_subnet.private_subnet: Creation complete after 0s [id=3537577-10.0.0.0/16]
â•·
â”‚ Error: SSH key not found: access1
â”‚ 
â”‚   with module.base.hcloud_server.bastion,
â”‚   on ../../modules/base/main.tf line 16, in resource "hcloud_server" "bastion":
â”‚   16: resource "hcloud_server" "bastion" {
â”‚ 
â•µ
IP is 
@FaycalD âžœ /workspaces/onecluster (main) $ ./run.sh deploy base
module.base.hcloud_network.private_net: Refreshing state... [id=3537577]
module.base.hcloud_network_subnet.private_subnet: Refreshing state... [id=3537577-10.0.0.0/16]

Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:
  + create

Terraform will perform the following actions:

  # module.base.hcloud_server.bastion will be created
  + resource "hcloud_server" "bastion" {
      + allow_deprecated_images    = false
      + backup_window              = (known after apply)
      + backups                    = false
      + datacenter                 = (known after apply)
      + delete_protection          = false
      + firewall_ids               = (known after apply)
      + id                         = (known after apply)
      + ignore_remote_firewall_ids = false
      + image                      = "ubuntu-20.04"
      + ipv4_address               = (known after apply)
      + ipv6_address               = (known after apply)
      + ipv6_network               = (known after apply)
      + keep_disk                  = false
      + location                   = "nbg1"
      + name                       = "bastion"
      + rebuild_protection         = false
      + server_type                = "cx31"
      + shutdown_before_deletion   = false
      + ssh_keys                   = [
          + "access1",
        ]
      + status                     = (known after apply)
    }

  # module.base.hcloud_server_network.server_network_bastion will be created
  + resource "hcloud_server_network" "server_network_bastion" {
      + id          = (known after apply)
      + ip          = "10.0.0.2"
      + mac_address = (known after apply)
      + network_id  = 3537577
      + server_id   = (known after apply)
    }

Plan: 2 to add, 0 to change, 0 to destroy.

Changes to Outputs:
  + bastion_ip = (known after apply)
module.base.hcloud_server.bastion: Creating...
â•·
â”‚ Error: SSH key not found: access1
â”‚ 
â”‚   with module.base.hcloud_server.bastion,
â”‚   on ../../modules/base/main.tf line 16, in resource "hcloud_server" "bastion":
â”‚   16: resource "hcloud_server" "bastion" {
â”‚ 
â•µ
IP is 
@FaycalD âžœ /workspaces/onecluster (main) $ terraform destroy

No changes. No objects need to be destroyed.

Either you have not created any objects yet or the existing objects were already deleted outside of Terraform.

Destroy complete! Resources: 0 destroyed.
@FaycalD âžœ /workspaces/onecluster (main) $ ./run.sh destroy base
module.base.hcloud_network.private_net: Refreshing state... [id=3537577]
module.base.hcloud_network_subnet.private_subnet: Refreshing state... [id=3537577-10.0.0.0/16]

Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:
  - destroy

Terraform will perform the following actions:

  # module.base.hcloud_network.private_net will be destroyed
  - resource "hcloud_network" "private_net" {
      - delete_protection        = false -> null
      - expose_routes_to_vswitch = false -> null
      - id                       = "3537577" -> null
      - ip_range                 = "10.0.0.0/16" -> null
      - labels                   = {
          - "name" = "mycluster-main"
        } -> null
      - name                     = "mycluster-main" -> null
    }

  # module.base.hcloud_network_subnet.private_subnet will be destroyed
  - resource "hcloud_network_subnet" "private_subnet" {
      - gateway      = "10.0.0.1" -> null
      - id           = "3537577-10.0.0.0/16" -> null
      - ip_range     = "10.0.0.0/16" -> null
      - network_id   = 3537577 -> null
      - network_zone = "eu-central" -> null
      - type         = "server" -> null
    }

Plan: 0 to add, 0 to change, 2 to destroy.
module.base.hcloud_network_subnet.private_subnet: Destroying... [id=3537577-10.0.0.0/16]
module.base.hcloud_network_subnet.private_subnet: Destruction complete after 1s
module.base.hcloud_network.private_net: Destroying... [id=3537577]
module.base.hcloud_network.private_net: Destruction complete after 0s

Destroy complete! Resources: 2 destroyed.
@FaycalD âžœ /workspaces/onecluster (main) $ ./run.sh init base

Initializing the backend...
Initializing modules...

Initializing provider plugins...
- Reusing previous version of hetznercloud/hcloud from the dependency lock file
- Using previously-installed hetznercloud/hcloud v1.44.1

Terraform has been successfully initialized!

You may now begin working with Terraform. Try running "terraform plan" to see
any changes that are required for your infrastructure. All Terraform commands
should now work.

If you ever set or change modules or backend configuration for Terraform,
rerun this command to reinitialize your working directory. If you forget, other
commands will detect it and remind you to do so if necessary.
@FaycalD âžœ /workspaces/onecluster (main) $ ./run.sh deploy base

Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:
  + create

Terraform will perform the following actions:

  # module.base.hcloud_network.private_net will be created
  + resource "hcloud_network" "private_net" {
      + delete_protection        = false
      + expose_routes_to_vswitch = false
      + id                       = (known after apply)
      + ip_range                 = "10.0.0.0/16"
      + labels                   = {
          + "name" = "mycluster-main"
        }
      + name                     = "mycluster-main"
    }

  # module.base.hcloud_network_subnet.private_subnet will be created
  + resource "hcloud_network_subnet" "private_subnet" {
      + gateway      = (known after apply)
      + id           = (known after apply)
      + ip_range     = "10.0.0.0/16"
      + network_id   = (known after apply)
      + network_zone = "eu-central"
      + type         = "server"
    }

  # module.base.hcloud_server.bastion will be created
  + resource "hcloud_server" "bastion" {
      + allow_deprecated_images    = false
      + backup_window              = (known after apply)
      + backups                    = false
      + datacenter                 = (known after apply)
      + delete_protection          = false
      + firewall_ids               = (known after apply)
      + id                         = (known after apply)
      + ignore_remote_firewall_ids = false
      + image                      = "ubuntu-20.04"
      + ipv4_address               = (known after apply)
      + ipv6_address               = (known after apply)
      + ipv6_network               = (known after apply)
      + keep_disk                  = false
      + location                   = "nbg1"
      + name                       = "bastion"
      + rebuild_protection         = false
      + server_type                = "cx31"
      + shutdown_before_deletion   = false
      + ssh_keys                   = [
          + "access1",
        ]
      + status                     = (known after apply)
    }

  # module.base.hcloud_server_network.server_network_bastion will be created
  + resource "hcloud_server_network" "server_network_bastion" {
      + id          = (known after apply)
      + ip          = "10.0.0.2"
      + mac_address = (known after apply)
      + network_id  = (known after apply)
      + server_id   = (known after apply)
    }

Plan: 4 to add, 0 to change, 0 to destroy.

Changes to Outputs:
  + bastion_ip = (known after apply)
module.base.hcloud_network.private_net: Creating...
module.base.hcloud_server.bastion: Creating...
module.base.hcloud_network.private_net: Creation complete after 2s [id=3537605]
module.base.hcloud_network_subnet.private_subnet: Creating...
module.base.hcloud_network_subnet.private_subnet: Creation complete after 1s [id=3537605-10.0.0.0/16]
â•·
â”‚ Error: SSH key not found: access1
â”‚ 
â”‚   with module.base.hcloud_server.bastion,
â”‚   on ../../modules/base/main.tf line 16, in resource "hcloud_server" "bastion":
â”‚   16: resource "hcloud_server" "bastion" {
â”‚ 
â•µ
IP is 
@FaycalD âžœ /workspaces/onecluster (main) $ ./run.sh destroy base
module.base.hcloud_network.private_net: Refreshing state... [id=3537605]
module.base.hcloud_network_subnet.private_subnet: Refreshing state... [id=3537605-10.0.0.0/16]

Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:
  - destroy

Terraform will perform the following actions:

  # module.base.hcloud_network.private_net will be destroyed
  - resource "hcloud_network" "private_net" {
      - delete_protection        = false -> null
      - expose_routes_to_vswitch = false -> null
      - id                       = "3537605" -> null
      - ip_range                 = "10.0.0.0/16" -> null
      - labels                   = {
          - "name" = "mycluster-main"
        } -> null
      - name                     = "mycluster-main" -> null
    }

  # module.base.hcloud_network_subnet.private_subnet will be destroyed
  - resource "hcloud_network_subnet" "private_subnet" {
      - gateway      = "10.0.0.1" -> null
      - id           = "3537605-10.0.0.0/16" -> null
      - ip_range     = "10.0.0.0/16" -> null
      - network_id   = 3537605 -> null
      - network_zone = "eu-central" -> null
      - type         = "server" -> null
    }

Plan: 0 to add, 0 to change, 2 to destroy.
module.base.hcloud_network_subnet.private_subnet: Destroying... [id=3537605-10.0.0.0/16]
module.base.hcloud_network_subnet.private_subnet: Destruction complete after 1s
module.base.hcloud_network.private_net: Destroying... [id=3537605]
module.base.hcloud_network.private_net: Destruction complete after 1s

Destroy complete! Resources: 2 destroyed.
@FaycalD âžœ /workspaces/onecluster (main) $ ./run.sh init base

Initializing the backend...
Initializing modules...

Initializing provider plugins...
- Reusing previous version of hetznercloud/hcloud from the dependency lock file
- Using previously-installed hetznercloud/hcloud v1.44.1

Terraform has been successfully initialized!

You may now begin working with Terraform. Try running "terraform plan" to see
any changes that are required for your infrastructure. All Terraform commands
should now work.

If you ever set or change modules or backend configuration for Terraform,
rerun this command to reinitialize your working directory. If you forget, other
commands will detect it and remind you to do so if necessary.
@FaycalD âžœ /workspaces/onecluster (main) $ ./run.sh deploy base

Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:
  + create

Terraform will perform the following actions:

  # module.base.hcloud_network.private_net will be created
  + resource "hcloud_network" "private_net" {
      + delete_protection        = false
      + expose_routes_to_vswitch = false
      + id                       = (known after apply)
      + ip_range                 = "10.0.0.0/16"
      + labels                   = {
          + "name" = "mycluster-main"
        }
      + name                     = "mycluster-main"
    }

  # module.base.hcloud_network_subnet.private_subnet will be created
  + resource "hcloud_network_subnet" "private_subnet" {
      + gateway      = (known after apply)
      + id           = (known after apply)
      + ip_range     = "10.0.0.0/16"
      + network_id   = (known after apply)
      + network_zone = "eu-central"
      + type         = "server"
    }

  # module.base.hcloud_server.bastion will be created
  + resource "hcloud_server" "bastion" {
      + allow_deprecated_images    = false
      + backup_window              = (known after apply)
      + backups                    = false
      + datacenter                 = (known after apply)
      + delete_protection          = false
      + firewall_ids               = (known after apply)
      + id                         = (known after apply)
      + ignore_remote_firewall_ids = false
      + image                      = "ubuntu-20.04"
      + ipv4_address               = (known after apply)
      + ipv6_address               = (known after apply)
      + ipv6_network               = (known after apply)
      + keep_disk                  = false
      + location                   = "nbg1"
      + name                       = "bastion"
      + rebuild_protection         = false
      + server_type                = "cx31"
      + shutdown_before_deletion   = false
      + ssh_keys                   = [
          + "accesskey",
        ]
      + status                     = (known after apply)
    }

  # module.base.hcloud_server_network.server_network_bastion will be created
  + resource "hcloud_server_network" "server_network_bastion" {
      + id          = (known after apply)
      + ip          = "10.0.0.2"
      + mac_address = (known after apply)
      + network_id  = (known after apply)
      + server_id   = (known after apply)
    }

Plan: 4 to add, 0 to change, 0 to destroy.

Changes to Outputs:
  + bastion_ip = (known after apply)
module.base.hcloud_network.private_net: Creating...
module.base.hcloud_server.bastion: Creating...
module.base.hcloud_network.private_net: Creation complete after 0s [id=3537606]
module.base.hcloud_network_subnet.private_subnet: Creating...
module.base.hcloud_network_subnet.private_subnet: Creation complete after 1s [id=3537606-10.0.0.0/16]
module.base.hcloud_server.bastion: Still creating... [10s elapsed]
module.base.hcloud_server.bastion: Creation complete after 10s [id=39120578]
module.base.hcloud_server_network.server_network_bastion: Creating...
module.base.hcloud_server_network.server_network_bastion: Still creating... [10s elapsed]
module.base.hcloud_server_network.server_network_bastion: Creation complete after 19s [id=39120578-3537606]

Apply complete! Resources: 4 added, 0 changed, 0 destroyed.

Outputs:

bastion_ip = "157.90.18.194"
IP is 157.90.18.194
@FaycalD âžœ /workspaces/onecluster (main) $ ssh -i ~/.ssh/tcloud root@157.90.18.194
The authenticity of host '157.90.18.194 (157.90.18.194)' can't be established.
ECDSA key fingerprint is SHA256:d5s2pYy6NdQ14js0e9Nhrvx17bfSgY7gfWQ2P9wvcWE.
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
Warning: Permanently added '157.90.18.194' (ECDSA) to the list of known hosts.
Welcome to Ubuntu 20.04.6 LTS (GNU/Linux 5.4.0-166-generic x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

  System information as of Thu 09 Nov 2023 12:23:07 AM UTC

  System load:  0.12              Users logged in:        0
  Usage of /:   2.0% of 74.80GB   IPv4 address for ens10: 10.0.0.2
  Memory usage: 2%                IPv4 address for eth0:  157.90.18.194
  Swap usage:   0%                IPv6 address for eth0:  2a01:4f8:c0c:ee08::1
  Processes:    122


Expanded Security Maintenance for Applications is not enabled.

0 updates can be applied immediately.

Enable ESM Apps to receive additional future security updates.
See https://ubuntu.com/esm or run: sudo pro status

New release '22.04.3 LTS' available.
Run 'do-release-upgrade' to upgrade to it.


root@bastion:~# pwd
/root
root@bastion:~# ls -al
total 24
drwx------  4 root root 4096 Nov  8 11:21 .
drwxr-xr-x 18 root root 4096 Nov  9 00:21 ..
-rw-r--r--  1 root root 3106 Dec  5  2019 .bashrc
drwx------  2 root root 4096 Nov  8 11:17 .cache
-rw-r--r--  1 root root    0 Nov  8 11:20 .cloud-locale-test.skip
-rw-r--r--  1 root root  161 Dec  5  2019 .profile
drwx------  2 root root 4096 Nov  8 11:17 .ssh
root@bastion:~# /workspaces/onecluster/1-infra/mycluster/prod
-bash: /workspaces/onecluster/1-infra/mycluster/prod: No such file or directory
root@bastion:~# exit
logout
Connection to 157.90.18.194 closed.
@FaycalD âžœ /workspaces/onecluster (main) $ ls
1-infra  2-provision  run.sh  terraform.tfstate
@FaycalD âžœ /workspaces/onecluster (main) $ cd 2-provision/
@FaycalD âžœ /workspaces/onecluster/2-provision (main) $ cd mycluster/prod
bash: cd: mycluster/prod: No such file or directory
@FaycalD âžœ /workspaces/onecluster/2-provision (main) $ terraform init
Terraform initialized in an empty directory!

The directory has no Terraform configuration files. You may begin working
with Terraform immediately by creating Terraform configuration files.
@FaycalD âžœ /workspaces/onecluster/2-provision (main) $ cd ..
@FaycalD âžœ /workspaces/onecluster (main) $ cd 1-infra/
@FaycalD âžœ /workspaces/onecluster/1-infra (main) $ cd mycluster/prod
@FaycalD âžœ .../onecluster/1-infra/mycluster/prod (main) $ terraform init

Initializing the backend...
Initializing modules...
- cluster in ../../modules/hcloud

Initializing provider plugins...
- Finding latest version of hetznercloud/hcloud...
- Installing hetznercloud/hcloud v1.44.1...
- Installed hetznercloud/hcloud v1.44.1 (signed by a HashiCorp partner, key ID 5219EACB3A77198B)

Partner and community providers are signed by their developers.
If you'd like to know more about provider signing, you can read about it here:
https://www.terraform.io/docs/cli/plugins/signing.html

Terraform has created a lock file .terraform.lock.hcl to record the provider
selections it made above. Include this file in your version control repository
so that Terraform can guarantee to make the same selections by default when
you run "terraform init" in the future.

Terraform has been successfully initialized!

You may now begin working with Terraform. Try running "terraform plan" to see
any changes that are required for your infrastructure. All Terraform commands
should now work.

If you ever set or change modules or backend configuration for Terraform,
rerun this command to reinitialize your working directory. If you forget, other
commands will detect it and remind you to do so if necessary.
@FaycalD âžœ .../onecluster/1-infra/mycluster/prod (main) $ ./run.sh deploy base^C
@FaycalD âžœ .../onecluster/1-infra/mycluster/prod (main) $ cd ..
@FaycalD âžœ /workspaces/onecluster/1-infra/mycluster (main) $ cd ..
@FaycalD âžœ /workspaces/onecluster/1-infra (main) $ cd ..
@FaycalD âžœ /workspaces/onecluster (main) $ ls
1-infra  2-provision  run.sh  terraform.tfstate
@FaycalD âžœ /workspaces/onecluster (main) $ ./run.sh deploy prod
module.cluster.data.hcloud_network.private_net: Reading...
module.cluster.data.hcloud_network.private_net: Read complete after 1s [name=mycluster-main]

Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the
following symbols:
  + create

Terraform will perform the following actions:

  # module.cluster.hcloud_load_balancer.load_balancer will be created
  + resource "hcloud_load_balancer" "load_balancer" {
      + delete_protection  = false
      + id                 = (known after apply)
      + ipv4               = (known after apply)
      + ipv6               = (known after apply)
      + labels             = (known after apply)
      + load_balancer_type = "lb11"
      + location           = "nbg1"
      + name               = "prod-lb"
      + network_id         = (known after apply)
      + network_ip         = (known after apply)
      + network_zone       = (known after apply)

      + target {
          + server_id      = (known after apply)
          + type           = "server"
          + use_private_ip = false
        }
      + target {
          + server_id      = (known after apply)
          + type           = "server"
          + use_private_ip = false
        }
      + target {
          + server_id      = (known after apply)
          + type           = "server"
          + use_private_ip = false
        }
    }

  # module.cluster.hcloud_load_balancer_network.server_network_lb will be created
  + resource "hcloud_load_balancer_network" "server_network_lb" {
      + enable_public_interface = true
      + id                      = (known after apply)
      + ip                      = "10.0.0.3"
      + load_balancer_id        = (known after apply)
      + network_id              = 3537606
    }

  # module.cluster.hcloud_server.cloud_nodes["1"] will be created
  + resource "hcloud_server" "cloud_nodes" {
      + allow_deprecated_images    = false
      + backup_window              = (known after apply)
      + backups                    = false
      + datacenter                 = (known after apply)
      + delete_protection          = false
      + firewall_ids               = (known after apply)
      + id                         = (known after apply)
      + ignore_remote_firewall_ids = false
      + image                      = "ubuntu-20.04"
      + ipv4_address               = (known after apply)
      + ipv6_address               = (known after apply)
      + ipv6_network               = (known after apply)
      + keep_disk                  = false
      + location                   = "nbg1"
      + name                       = "node1"
      + rebuild_protection         = false
      + server_type                = "cx21"
      + shutdown_before_deletion   = false
      + ssh_keys                   = [
          + "accesskey",
        ]
      + status                     = (known after apply)
    }

  # module.cluster.hcloud_server.cloud_nodes["2"] will be created
  + resource "hcloud_server" "cloud_nodes" {
      + allow_deprecated_images    = false
      + backup_window              = (known after apply)
      + backups                    = false
      + datacenter                 = (known after apply)
      + delete_protection          = false
      + firewall_ids               = (known after apply)
      + id                         = (known after apply)
      + ignore_remote_firewall_ids = false
      + image                      = "ubuntu-20.04"
      + ipv4_address               = (known after apply)
      + ipv6_address               = (known after apply)
      + ipv6_network               = (known after apply)
      + keep_disk                  = false
      + location                   = "nbg1"
      + name                       = "node2"
      + rebuild_protection         = false
      + server_type                = "cx21"
      + shutdown_before_deletion   = false
      + ssh_keys                   = [
          + "accesskey",
        ]
      + status                     = (known after apply)
    }

  # module.cluster.hcloud_server.cloud_nodes["3"] will be created
  + resource "hcloud_server" "cloud_nodes" {
      + allow_deprecated_images    = false
      + backup_window              = (known after apply)
      + backups                    = false
      + datacenter                 = (known after apply)
      + delete_protection          = false
      + firewall_ids               = (known after apply)
      + id                         = (known after apply)
      + ignore_remote_firewall_ids = false
      + image                      = "ubuntu-20.04"
      + ipv4_address               = (known after apply)
      + ipv6_address               = (known after apply)
      + ipv6_network               = (known after apply)
      + keep_disk                  = false
      + location                   = "nbg1"
      + name                       = "node3"
      + rebuild_protection         = false
      + server_type                = "cx21"
      + shutdown_before_deletion   = false
      + ssh_keys                   = [
          + "accesskey",
        ]
      + status                     = (known after apply)
    }

  # module.cluster.hcloud_server_network.server_network["1"] will be created
  + resource "hcloud_server_network" "server_network" {
      + id          = (known after apply)
      + ip          = "10.0.0.5"
      + mac_address = (known after apply)
      + network_id  = 3537606
      + server_id   = (known after apply)
    }

  # module.cluster.hcloud_server_network.server_network["2"] will be created
  + resource "hcloud_server_network" "server_network" {
      + id          = (known after apply)
      + ip          = "10.0.0.6"
      + mac_address = (known after apply)
      + network_id  = 3537606
      + server_id   = (known after apply)
    }

  # module.cluster.hcloud_server_network.server_network["3"] will be created
  + resource "hcloud_server_network" "server_network" {
      + id          = (known after apply)
      + ip          = "10.0.0.7"
      + mac_address = (known after apply)
      + network_id  = 3537606
      + server_id   = (known after apply)
    }

  # module.cluster.hcloud_volume.volumes["1"] will be created
  + resource "hcloud_volume" "volumes" {
      + automount         = false
      + delete_protection = false
      + id                = (known after apply)
      + linux_device      = (known after apply)
      + location          = (known after apply)
      + name              = "node1-volx"
      + server_id         = (known after apply)
      + size              = 10
    }

  # module.cluster.hcloud_volume.volumes["2"] will be created
  + resource "hcloud_volume" "volumes" {
      + automount         = false
      + delete_protection = false
      + id                = (known after apply)
      + linux_device      = (known after apply)
      + location          = (known after apply)
      + name              = "node2-volx"
      + server_id         = (known after apply)
      + size              = 10
    }

  # module.cluster.hcloud_volume.volumes["3"] will be created
  + resource "hcloud_volume" "volumes" {
      + automount         = false
      + delete_protection = false
      + id                = (known after apply)
      + linux_device      = (known after apply)
      + location          = (known after apply)
      + name              = "node3-volx"
      + server_id         = (known after apply)
      + size              = 10
    }

Plan: 11 to add, 0 to change, 0 to destroy.

Changes to Outputs:
  + load_balancer = (known after apply)
  + nodes         = {
      + "1" = {
          + internal_ip  = "10.0.0.5"
          + ipv4_address = (known after apply)
          + name         = "node1"
        }
      + "2" = {
          + internal_ip  = "10.0.0.6"
          + ipv4_address = (known after apply)
          + name         = "node2"
        }
      + "3" = {
          + internal_ip  = "10.0.0.7"
          + ipv4_address = (known after apply)
          + name         = "node3"
        }
    }
module.cluster.hcloud_server.cloud_nodes["1"]: Creating...
module.cluster.hcloud_server.cloud_nodes["3"]: Creating...
module.cluster.hcloud_server.cloud_nodes["2"]: Creating...
module.cluster.hcloud_server.cloud_nodes["1"]: Still creating... [10s elapsed]
module.cluster.hcloud_server.cloud_nodes["3"]: Still creating... [10s elapsed]
module.cluster.hcloud_server.cloud_nodes["2"]: Still creating... [10s elapsed]
module.cluster.hcloud_server.cloud_nodes["2"]: Creation complete after 13s [id=39120833]
module.cluster.hcloud_server.cloud_nodes["1"]: Creation complete after 13s [id=39120839]
module.cluster.hcloud_server.cloud_nodes["3"]: Creation complete after 14s [id=39120828]
module.cluster.hcloud_server_network.server_network["3"]: Creating...
module.cluster.hcloud_volume.volumes["2"]: Creating...
module.cluster.hcloud_volume.volumes["3"]: Creating...
module.cluster.hcloud_server_network.server_network["2"]: Creating...
module.cluster.hcloud_server_network.server_network["1"]: Creating...
module.cluster.hcloud_volume.volumes["1"]: Creating...
module.cluster.hcloud_load_balancer.load_balancer: Creating...
module.cluster.hcloud_load_balancer.load_balancer: Creation complete after 2s [id=1530302]
module.cluster.hcloud_load_balancer_network.server_network_lb: Creating...
module.cluster.hcloud_server_network.server_network["2"]: Creation complete after 3s [id=39120833-3537606]
module.cluster.hcloud_server_network.server_network["1"]: Creation complete after 3s [id=39120839-3537606]
module.cluster.hcloud_load_balancer_network.server_network_lb: Creation complete after 3s [id=1530302-3537606]
module.cluster.hcloud_volume.volumes["2"]: Still creating... [10s elapsed]
module.cluster.hcloud_volume.volumes["3"]: Still creating... [10s elapsed]
module.cluster.hcloud_server_network.server_network["3"]: Still creating... [10s elapsed]
module.cluster.hcloud_volume.volumes["1"]: Still creating... [10s elapsed]
module.cluster.hcloud_server_network.server_network["3"]: Creation complete after 18s [id=39120828-3537606]
module.cluster.hcloud_volume.volumes["3"]: Still creating... [20s elapsed]
module.cluster.hcloud_volume.volumes["2"]: Still creating... [20s elapsed]
module.cluster.hcloud_volume.volumes["1"]: Still creating... [20s elapsed]
module.cluster.hcloud_volume.volumes["2"]: Creation complete after 21s [id=100069219]
module.cluster.hcloud_volume.volumes["1"]: Creation complete after 22s [id=100069221]
module.cluster.hcloud_volume.volumes["3"]: Creation complete after 23s [id=100069220]
â•·
â”‚ Warning: Argument is deprecated
â”‚ 
â”‚   with module.cluster.hcloud_load_balancer.load_balancer,
â”‚   on ../../modules/hcloud/main.tf line 33, in resource "hcloud_load_balancer" "load_balancer":
â”‚   33: resource "hcloud_load_balancer" "load_balancer" {
â”‚ 
â”‚ Use hcloud_load_balancer_target resource instead. This allows the full control over the selected targets.
â•µ

Apply complete! Resources: 11 added, 0 changed, 0 destroyed.

Outputs:

load_balancer = "49.13.40.59"
nodes = {
  "1" = {
    "internal_ip" = "10.0.0.5"
    "ipv4_address" = "78.47.190.24"
    "name" = "node1"
  }
  "2" = {
    "internal_ip" = "10.0.0.6"
    "ipv4_address" = "167.235.135.134"
    "name" = "node2"
  }
  "3" = {
    "internal_ip" = "10.0.0.7"
    "ipv4_address" = "159.69.159.53"
    "name" = "node3"
  }
}
159.69.159.53
167.235.135.134
78.47.190.24
@FaycalD âžœ /workspaces/onecluster (main) $ 